## REPORT for Alphabet Soup foundation

1. **Overview** 
The goal of this project is to predict whether or not applicants for funding from the Alphabet Soup foundation will be successful. For this forecast, I am using a Deep Learning model with binary classifier based on more than 34,000 organizations. As an input to the model I use data such as application type, affiliated sector of industry, use case for funding, funding amount requested and more to determine if the money requested was used effectively or not.    

2. **Results**: 
As an initial step, I loaded and pre-processed data by sorting data between target and features for the model. I defined the target - `IS_SUCCESSFUL`, and features - `APPLICATION_TYPE`, `AFFILIATION`, `CLASSIFICATION`, `USE_CASE`, `ORGANIZATION`, `STATUS`, `INCOME_AMT`, `SPECIAL_CONSIDERATIONS`, `ASK_AMT` for my model. Columns `EIN` and `NAME` were dropped as non-beneficial. For the features that had more than 10 unique values, I reduced the number of bins (number of unique values) for `APPLICATION_TYPE` (from 17 to 9) and `CLASSIFICATION` (from 71 to 6) before I encoded these categorical variables using get_dummies. Then, I designed a Deep Learning (using TensorFlow Keras) that can predict if an Alphabet Soup-funded organization will be successful based on the features. After that, I compiled, trained, and evaluated the model. My initial setup had 3 hidden layers: ReLU activation function / 10 neurons, ReLU activation function / 12 neurons, Tanh activation function / 6 neurons. For the output layer, I used the Sigmoid activation function with 1 neuron. I set epochs at 100. Using this setup, my model produced an accuracy score of 0.7259 and a loss of 0.5519. These numbers were not great, and the model needed to be improved. 
As an optimization step, the goal was to achieve a target predictive accuracy higher than 75%. I tried many different changes/combinations, removing features, increasing/decreasing number of bins, adding/removing hidden layers, changing activation functions, adding/removing neurons, increasing decreasing number of epochs, but was not able to achieve any noticeable accuracy improvement, and could not reach 75%. So, I decided to bring back the column `NAME` as a feature and input it into my model. This change made more difference - the accuracy score improved and the loss went down. Then, I made some additional adjustments (see below), my model reached an accuracy of 0.7950 and a loss of 0.4391. Comparing to initial setup, the target predictive accuracy of 75% was achieved. However, these are still not spectacular statistics as a good model would have more than 90% accuracy.
Optimized model summary:
Features - `NAME` (reduced to 578 bins), `APPLICATION_TYPE` (reduced to 8 bins), `CLASSIFICATION` (reduced to 6 bins), `USE_CASE`, `STATUS`, `INCOME_AMT`, `ASK_AMT`. Dropped `EIN`, `AFFILIATION`, `ORGANIZATION`, `SPECIAL_CONSIDERATIONS`.
Target - `IS_SUCCESSFUL`
Layers - ReLU activation function / 80 neurons; ReLU activation function / 30 neurons; Dropout layer (0.2); ReLU activation function / 12 neurons; Tanh activation function / 8 neurons.
Output layer - Sigmoid activation function / 1 neuron.

3. **Summary**: 
My model got better results when I didn't remove the `NAME` column, it achieved almost 80% accuracy. Even though we are looking for a better accuracy score, bringing back `NAME` feature (initially was dropped) had good impact on the result (with `NAME` - accuracy score is 0.7950; without - 0.7259). The model now can predict with almost 80% accuracy that the Alphabet Soup funded organizations will be successful or not. 